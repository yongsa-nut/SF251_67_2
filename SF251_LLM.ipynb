{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON3iUrjr2bCkodyj12vqqw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsa-nut/SF251_67_2/blob/main/SF251_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cohere API and LLM Basic"
      ],
      "metadata": {
        "id": "NO9W08JgLcSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "id": "5k5uVRTXf7tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import cohere\n",
        "\n",
        "# Get your free API key: https://dashboard.cohere.com/api-keys\n",
        "co = cohere.ClientV2(api_key=userdata.get('cohere'))"
      ],
      "metadata": {
        "id": "qy36i-a5dKYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic API Usage"
      ],
      "metadata": {
        "id": "fXnJ6niyglSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the user message\n",
        "message = \"Hello!\"\n",
        "\n",
        "# Add the messages\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": message}\n",
        "]\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-a-03-2025\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ],
      "metadata": {
        "id": "DkerjiXBfzRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDQXWqVqipY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other example: https://openrouter.ai/"
      ],
      "metadata": {
        "id": "iZQy1dfEgrGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation Loop:\n",
        "- User -> Assistant -> User -> Assistant -> ..."
      ],
      "metadata": {
        "id": "ZZxffz-Jg9KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"\n",
        "\n",
        "# Create a custom system message\n",
        "system_message = \"\"\"## Task and Context\n",
        "Generate concise responses, with maximum one-sentence.\"\"\"\n",
        "\n",
        "# Add the messages\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "]\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-a-03-2025\", messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ],
      "metadata": {
        "id": "rPQ82-OSdKiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then add the assistant's response to our messages and we can add the user's message next."
      ],
      "metadata": {
        "id": "E1c2pZAzhQ89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append the previous response\n",
        "messages.append(\n",
        "    {\"role\": \"assistant\", \"content\": response.message.content[0].text}\n",
        ")\n",
        "# Add the user message\n",
        "message = \"Make it more upbeat and conversational.\"\n",
        "# Append the user message\n",
        "messages.append({\"role\": \"user\", \"content\": message})\n",
        "# Generate the response with the current chat history as the context\n",
        "response = co.chat(model=\"command-a-03-2025\", messages=messages)\n",
        "print(response.message.content[0].text)"
      ],
      "metadata": {
        "id": "ieAz4j-RdK4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can keep going.\n",
        "Below is the full loop."
      ],
      "metadata": {
        "id": "-X0Jaw7ohURH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are a helpful assistant.\" # << Change this to see the effect\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "# Coding this together!\n",
        "\n"
      ],
      "metadata": {
        "id": "XRjpk3kShXlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "id": "BYf2bmDejl3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG\n",
        "\n",
        "- Note: Cohere actually has its own RAG api ([documentation](https://docs.cohere.com/docs/rag-with-cohere))"
      ],
      "metadata": {
        "id": "EnNhaThHdMcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "## Demo 1: Keyword Matching"
      ],
      "metadata": {
        "id": "RCHIjOUTLfUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(query):\n",
        "  response = co.chat(model=\"command-a-03-2025\",\n",
        "                     messages=[{'role':'user','content':query}])\n",
        "  return response.message.content[0].text\n",
        "\n",
        "get_response('Hello test test')"
      ],
      "metadata": {
        "id": "8O1mXG2fjFyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data\n",
        "lorebook = { 'ตึกวิจัย' : 'ตำแหน่ง latitude = 14.0694899, longitude = 100.6050282',\n",
        "             'ตึกอำนวยการ' : 'ตำแหน่ง latitude = 14.0690024, longitude = 100.6061611'\n",
        "}"
      ],
      "metadata": {
        "id": "KR6PDnuTLqPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_generate(query, docs):\n",
        "  # Retrive information and create context\n",
        "  context = '<context>'\n",
        "  for k in docs:\n",
        "    if k in query:\n",
        "      context += f'{k} = {docs[k]}\\n'\n",
        "  context += '</context>'\n",
        "\n",
        "  prompt = f'''<question>{query}</question>\n",
        "  Please use context in <context> tags to answer the question.\n",
        "  <context>{context}</context>'''\n",
        "  response = get_response(prompt)\n",
        "  # printing out\n",
        "  print('Query:', query)\n",
        "  print('Retrieved documents:', context)\n",
        "  print('Response:', response)\n",
        "\n",
        "query = 'ตึกวิจัยอยู่ไหน?'\n",
        "\n",
        "keyword_generate(query, lorebook)"
      ],
      "metadata": {
        "id": "Oh9tZBohNMy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acedc08e-30a5-4beb-8d21-dea4d3433d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: ตึกวิจัยอยู่ไหน?\n",
            "Retrieved documents: <context>ตึกวิจัย = ตำแหน่ง latitude = 14.0694899, longitude = 100.6050282\n",
            "</context>\n",
            "Response: ตึกวิจัยอยู่ที่ตำแหน่ง latitude 14.0694899, longitude 100.6050282\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "## Demo 2: BM25"
      ],
      "metadata": {
        "id": "d4YUIn8vQUsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install rank_bm25"
      ],
      "metadata": {
        "id": "SeGE2QcJc5P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "bLLcL3dGQXO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document collection\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"All that glitters is not gold.\",\n",
        "    \"Where there's smoke, there's fire.\",\n",
        "    \"The early bird catches the worm.\",\n",
        "    \"Actions speak louder than words.\",\n",
        "    \"Knowledge is power, but enthusiasm pulls the switch.\",\n",
        "    \"The pen is mightier than the sword.\",\n",
        "    \"When life gives you lemons, make lemonade.\"\n",
        "]\n",
        "\n",
        "# Simple tokenization using string splitting\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "# Create BM25 object\n",
        "bm25 = BM25Okapi(tokenized_docs)"
      ],
      "metadata": {
        "id": "4yfWslkwdZ-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_k=2):\n",
        "  # Words only\n",
        "  tokenized_query = query.lower().split()\n",
        "  # Pass the list of words into bm25 to get scores\n",
        "  doc_scores = bm25.get_scores(tokenized_query)\n",
        "  # Then retrieve the score\n",
        "  top_docs = sorted(enumerate(doc_scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "  return [documents[i] for i, _ in top_docs]"
      ],
      "metadata": {
        "id": "QRjJv2O5dvwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_response(query):\n",
        "  retrieved_docs = retrieve(query)\n",
        "  context = \"\\n\".join(retrieved_docs)\n",
        "\n",
        "  prompt = f'''<question>{query}</question>\n",
        "  Please use context in <context> tags to answr the question.\n",
        "  <context>{context}</context>'''\n",
        "  response = get_response(prompt)\n",
        "  # printing out\n",
        "  print('Query:', query)\n",
        "  print('Retrieved documents:', retrieved_docs)\n",
        "  print('Response:', response)\n",
        "\n",
        "query = \"What jumps over the dog?\"\n",
        "bm25_response(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jclDtDltd2aX",
        "outputId": "7c05f6e0-55de-4462-c14a-fcd52363de45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What jumps over the dog?\n",
            "Retrieved documents: ['The quick brown fox jumps over the lazy dog.', 'A journey of a thousand miles begins with a single step.']\n",
            "Response: The quick brown fox jumps over the dog.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "## Demo 3: RAG without actual database"
      ],
      "metadata": {
        "id": "9OKu_c9XQXg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers datasets"
      ],
      "metadata": {
        "id": "WBgAHs-G2rxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "EeZPGTOglxCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same document collection\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"All that glitters is not gold.\",\n",
        "    \"Where there's smoke, there's fire.\",\n",
        "    \"The early bird catches the worm.\",\n",
        "    \"Actions speak louder than words.\",\n",
        "    \"Knowledge is power, but enthusiasm pulls the switch.\",\n",
        "    \"The pen is mightier than the sword.\",\n",
        "    \"When life gives you lemons, make lemonade.\"\n",
        "]\n",
        "\n",
        "# Initialize sentence transformer model\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Embed documents\n",
        "doc_embeddings = embed_model.encode(documents)"
      ],
      "metadata": {
        "id": "cdMfAmJiQb_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embeddings"
      ],
      "metadata": {
        "id": "7dteCYnrD54u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc_embeddings[0])"
      ],
      "metadata": {
        "id": "dcsDYhKnD-0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec13df9e-518c-46ef-f9af-d10c36a05e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_RAG_docs(query, top_k=1):\n",
        "    # Embed the query\n",
        "    query_embedding = embed_model.encode([query])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "    # Get top-k relevant documents\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    return [documents[i] for i in top_indices]"
      ],
      "metadata": {
        "id": "gJeYSxVXl4p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG_response(query, top_k=3):\n",
        "  retrieved_docs = retrieve_RAG_docs(query, top_k)\n",
        "  context = \"\\n\".join(retrieved_docs)\n",
        "\n",
        "  prompt = f'''<question>{query}</question>\n",
        "  Please use context in <context> tags to answr the question.\n",
        "  <context>{context}</context>'''\n",
        "  response = get_response(prompt)\n",
        "  # printing out\n",
        "  print('Query:', query)\n",
        "  print('Retrieved documents:', retrieved_docs)\n",
        "  print('Response:', response)\n",
        "\n",
        "query = \"What jumps over the dog?\"\n",
        "RAG_response(query)"
      ],
      "metadata": {
        "id": "PMjsoZmGl-4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "## Demo 4: RAG with Pinecone"
      ],
      "metadata": {
        "id": "3xFmli7GHKNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pinecone"
      ],
      "metadata": {
        "id": "DcvarCOIHOri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec"
      ],
      "metadata": {
        "id": "O01aRLUcHPxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device != 'cuda':\n",
        "    print('Sorry no cuda.')\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
      ],
      "metadata": {
        "id": "7bN7jN1EHTFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What jumps over the dog?'\n",
        "xq = model.encode(query)\n",
        "xq.shape"
      ],
      "metadata": {
        "id": "IL2GZ9lbHWwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same document collection\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"All that glitters is not gold.\",\n",
        "    \"Where there's smoke, there's fire.\",\n",
        "    \"The early bird catches the worm.\",\n",
        "    \"Actions speak louder than words.\",\n",
        "    \"Knowledge is power, but enthusiasm pulls the switch.\",\n",
        "    \"The pen is mightier than the sword.\",\n",
        "    \"When life gives you lemons, make lemonade.\"\n",
        "]\n",
        "\n",
        "doc_embeddings = embed_model.encode(documents)"
      ],
      "metadata": {
        "id": "2wfKlSD-Hery"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Pinecone"
      ],
      "metadata": {
        "id": "ZzxE0W-KHaIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get secret key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "5u_ryrakNHC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone = Pinecone(api_key=userdata.get('pinecone_key'))\n",
        "INDEX_NAME = 'test1-2-16-2025'\n",
        "\n",
        "# Cleaning up the index\n",
        "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
        "    pinecone.delete_index(INDEX_NAME)\n",
        "print(INDEX_NAME)\n",
        "\n",
        "# Creating a serverless index\n",
        "pinecone.create_index(\n",
        "    name = INDEX_NAME,\n",
        "    dimension = model.get_sentence_embedding_dimension(),\n",
        "    metric = 'cosine',\n",
        "    spec = ServerlessSpec(cloud='aws', region='us-east-1')) #\n",
        "\n",
        "index = pinecone.Index(INDEX_NAME)\n",
        "print(index)"
      ],
      "metadata": {
        "id": "WnujxL-7HaIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upsert to Pinecone\n",
        "\n",
        "- Format: A list of dict\n",
        "  - `{'id':xx, 'value':embedding, 'metadata':dict}`\n",
        "- Document: https://docs.pinecone.io/reference/api/2024-07/data-plane/upsert"
      ],
      "metadata": {
        "id": "G7q4_ScGHpq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create id\n",
        "ids = [str(x) for x in range(len(documents))]\n",
        "# Create metadata\n",
        "metadatas = [{'text': text} for text in documents]\n",
        "# Zip them together\n",
        "records = zip(ids, doc_embeddings, metadatas)\n",
        "index.upsert(vectors=records)   # There is a limit on how much you can upsert at a time. See https://docs.pinecone.io/guides/data/upsert-data"
      ],
      "metadata": {
        "id": "XdWVB_pYHsJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53eaf222-c8b1-43bf-c98d-2149d60603fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'upserted_count': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gkXZ3xrIHysW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c7c56b-05d5-403b-f63d-4b14e5683e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriving Documents\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U2W1fv5UKhHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What jumps over the dog?'\n",
        "\n",
        "# 1) Embedding your query\n",
        "embed_query = model.encode(query).tolist()\n",
        "retrieved_docs =  index.query(vector=embed_query, top_k=1, include_metadata=True)\n",
        "print(retrieved_docs)"
      ],
      "metadata": {
        "id": "f9mGia8pKg1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ccd55a0-08c1-4bc3-d150-9b920323cff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'matches': [], 'namespace': '', 'usage': {'read_units': 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [r['metadata']['text'] for r in retrieved_docs['matches']]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "P5srP5e2L5q5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca90b5cd-77c3-48ce-eb54-cc021a7816d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate with retrived documents"
      ],
      "metadata": {
        "id": "wPdaAuH0KkDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG_pinecone_response(query, top_k=1):\n",
        "  # First embedding your query\n",
        "  embed_query = model.encode(query).tolist()\n",
        "  # Then retrieve the document\n",
        "  retrieved_docs =  index.query(vector=embed_query,\n",
        "                                top_k=top_k,\n",
        "                                include_metadata=True)\n",
        "  # Then get the actual text\n",
        "  text = [r['metadata']['text'] for r in retrieved_docs['matches']]\n",
        "  # Finally join them together\n",
        "  context = \"\\n\".join(text)\n",
        "\n",
        "  prompt = f'''<question>{query}</question>\n",
        "  Please use context in <context> tags to answr the question.\n",
        "  <context>{context}</context>'''\n",
        "  response = get_response(prompt)\n",
        "  # printing out\n",
        "  print('Query:', query)\n",
        "  print('Retrieved documents:', text)\n",
        "  print('Response:', response)\n",
        "\n",
        "query = 'What jumps over the dog?'\n",
        "RAG_pinecone_response(query)"
      ],
      "metadata": {
        "id": "llnGF5A5Kd4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}